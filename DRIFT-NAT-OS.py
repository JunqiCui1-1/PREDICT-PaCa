# -*- coding: utf-8 -*-
"""Untitled77.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tmvaBxkpCK70PzUHQ0h53_KL7V9CXe51
"""

#!/usr/bin/env python3
# =============================================================================
# Component 2 — DRIFT-NAT · Step 9 (OS integration for transport AIPW evaluation)
#
# GITHUB-STYLE HEADER
# -----------------------------------------------------------------------------
# DESCRIPTION
#   Integrate 1-year overall survival (OS@365) into DRIFT-NAT for transportable
#   causal policy evaluation on the target population.
#   - Prefer Cox PH (lifelines) per arm to estimate S_a(365 | X)
#   - Fallback: IPCW-weighted logistic regression to approximate P[Death<=365 | X, a]
#   - Evaluate individualized NAT-vs-US policy via g-transport AIPW
#   - Optional composite endpoint blending (OS + complications)
#
# INPUTS (CSV; relative to repo root)
#   - ./data/Chort_total.csv
#
# OUTPUTS (created under ./outputs/step9_os_<timestamp>/)
#   - tables/OS_risk_matrix_365d.csv            # OS event prob for each arm (US + 9 NAT arms)
#   - tables/policy_value_OS.csv                # policy value (mean, 95% CI) for DRIFT-NAT / US-only / Fixed-best
#   - tables/patient_level_OS_and_recommendation.csv
#   - figs/policy_value_OS.png
#   - meta_OS.json
#
# KEY DESIGN CHOICES
#   - Multi-arm NAT arms: {FFX, GemCap, CRT} × {<4w, 4–6w, >6w}
#   - Selection model s(X)=P(S=1|X) for transport weights (logistic)
#   - Arm propensity e_a(X) inside S=1 for NAT arms (multinomial logistic)
#   - OS risk per arm: Cox PH preferred; IPCW-Logit fallback
#   - Stage-1 tau-search for NAT-vs-US margin; Stage-2 best NAT arm by minimal LOSS
#   - Bootstrap to form 95% CI for transport policy value
#
# REQUIREMENTS (add to requirements.txt)
#   pandas, numpy, scikit-learn, matplotlib, scipy
#   (optional) lifelines         # to use Cox PH; otherwise IPCW-Logit fallback is used
#
# USAGE
#   python path/to/this_script.py
#   # Optional environment overrides:
#   #   PREDICT_PACA_TOTAL   # default: ./data/Chort_total.csv
#   #   PREDICT_PACA_OUTPUTS # default: ./outputs
# =============================================================================

import os, re, json, inspect
import numpy as np
import pandas as pd
from datetime import datetime
from typing import List

import matplotlib
import matplotlib.pyplot as plt

from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import GradientBoostingClassifier

# --- paths & dirs (GitHub-style relative paths) ---
DATA_CSV = os.environ.get("PREDICT_PACA_TOTAL", "./data/Chort_total.csv")
TS = datetime.now().strftime("%Y%m%d_%H%M%S")
OUTROOT = os.path.join(os.environ.get("PREDICT_PACA_OUTPUTS", "./outputs"), f"step9_os_{TS}")
os.makedirs(os.path.join(OUTROOT, "figs"), exist_ok=True)
os.makedirs(os.path.join(OUTROOT, "tables"), exist_ok=True)

# --- plotting defaults (no special fonts) ---
matplotlib.rcParams.update({"font.size": 11, "axes.labelsize": 11, "axes.titlesize": 12})

# --- configuration ---
RANDOM_STATE = 42
PI_CLIP = 1e-3
W_TRUNC_PCT = 0.99
MAX_CAT_UNIQUE = 50
TAU_GRID = np.round(np.linspace(0.00, 0.10, 11), 2)  # Stage-1 tau search
BOOT_B = 300

# NAT channel / regimen / timing candidates
NAT_CANDS = ["Neoadjuvant Therapy", "Neoadjuvant_therapy", "NAT", "Neoadjuvant"]
REGIMEN_COLS = {
    "FFX":    ["Folfirinox", "FOLFOXIRI", "FolfiriNox"],
    "GemCap": ["Gemcitabine Capecitabine", "GemCap", "Gemcitabine+Capecitabine"],
    "CRT":    ["Chemoradiotherapy", "Chemo-Radiotherapy", "Chemoradiation"]
}
TIMING_BIN_COLS = ["Less Than 4 Weeks", "Weeks 4-6", "Greater Than 6 Weeks"]
MDS_CANDS = ["Minimum Days To Surgery", "min_days_to_surgery", "Days To Surgery"]

# Composite complications (optional blend with OS)
OUTCOME_CANDS = {
    "Fistula": ["Fistula"],
    "Infection": ["Infection"],
    "Delayed Gastric Emptying": ["Delayed Gastrric Emptying", "Delayed Gastric Emptying"],
    "Death 90 Days": ["Death 90 Days", "Death 90 Day", "Death90Days"]
}
COMP_NAME = "Composite (Any of 4 Endpoints)"

# OS columns (time-to-event windowed to 365d)
TIME_CANDS  = ["Survival Days", "survival_days", "Days To Death"]
EVENT_CANDS = ["Death 365 Days", "death_365_days", "OS 1y", "Death within 365d"]

# Covariate exclusions (prevent leakage)
USER_EXCLUDE = {"Mean Arterial Pressure", "Hematocrit"}
MORE_EXCLUDE = {
    "Survival Days", "Death 365 Days",
    "Weeks 4-6", "Less Than 4 Weeks", "Greater Than 6 Weeks",
    "Minimum Days To Surgery", "min_days_to_surgery", "Days To Surgery",
    "Folfirinox", "Gemcitabine Capecitabine", "Chemoradiotherapy",
    "Neoadjuvant Therapy", "Neoadjuvant_therapy", "NAT", "Neoadjuvant",
}
ID_KEYS = {"id","subject_id","patient_id","mrn","patientunitstayid"}

# -----------------------------------------------------------------------------
# utils
# -----------------------------------------------------------------------------
def savefig(fig, path): fig.savefig(path, dpi=400, bbox_inches="tight"); plt.close(fig)

def read_csv_any(path):
    for enc in ("utf-8","utf-8-sig","latin1"):
        try: return pd.read_csv(path, encoding=enc, low_memory=False)
        except Exception: pass
    return pd.read_csv(path, low_memory=False)

def norm_name(s: str) -> str:
    s = str(s).lower().strip().replace("-", " ").replace("_", " ")
    return re.sub(r"\s+", " ", s)

def find_col_by_names(df_cols, candidates: List[str]):
    cnorms = [norm_name(c) for c in df_cols]
    for cand in candidates:
        if norm_name(cand) in cnorms:
            return df_cols[cnorms.index(norm_name(cand))]
    return None

def to01(series: pd.Series) -> np.ndarray:
    s = series.copy()
    if pd.api.types.is_numeric_dtype(s):
        vals = pd.to_numeric(s, errors="coerce").replace([np.inf,-np.inf], np.nan).fillna(0.0)
        uniq = set(np.unique(vals.dropna().astype(int)))
        if uniq <= {0,1}: return vals.astype(int).values
        return (vals > 0).astype(int).values
    ss = s.astype(str).str.strip().str.lower()
    return ss.isin({"1","true","t","yes","y"}).astype(int).values

def is_id_like(col: str) -> bool:
    lc = col.lower()
    return (lc in ID_KEYS) or lc.endswith("_id")

def make_ohe_kwargs():
    kw = {"handle_unknown": "ignore"}
    if "sparse_output" in inspect.signature(OneHotEncoder).parameters:
        kw["sparse_output"] = True
    else:
        kw["sparse"] = True
    return kw

def build_preprocessor(num_cols, cat_cols):
    ohe_kwargs = make_ohe_kwargs()
    num_pipe = Pipeline(steps=[("imputer", SimpleImputer(strategy="median")),
                               ("scaler", StandardScaler(with_mean=False))])
    cat_pipe = Pipeline(steps=[("imputer", SimpleImputer(strategy="most_frequent")),
                               ("onehot", OneHotEncoder(**ohe_kwargs))])
    return ColumnTransformer(
        transformers=[("num", num_pipe, num_cols), ("cat", cat_pipe, cat_cols)],
        remainder="drop", sparse_threshold=1.0
    )

def truncate_weights(w, pct=W_TRUNC_PCT):
    hi = np.quantile(w, pct); lo = np.quantile(w, 1.0-pct)
    return np.clip(w, lo, hi)

# -----------------------------------------------------------------------------
# load data
# -----------------------------------------------------------------------------
df = read_csv_any(DATA_CSV)

# S: NAT channel indicator (S=1 NAT, S=0 US)
nat_col = find_col_by_names(df.columns, NAT_CANDS)
if nat_col is None: raise ValueError("Column for 'Neoadjuvant Therapy' not found.")
df["S"] = to01(df[nat_col])

# regimen
def pick_regimen(row) -> str:
    hits = []
    for lab, cands in REGIMEN_COLS.items():
        col = find_col_by_names(df.columns, cands)
        if col is not None and row[col] == 1:
            hits.append(lab)
    return hits[0] if len(hits) > 0 else np.nan
df["regimen"] = df.apply(pick_regimen, axis=1)

# timing bin
def derive_timing(df_):
    cols_ok = [c for c in TIMING_BIN_COLS if c in df_.columns]
    if len(cols_ok) == 3:
        def pick(r):
            if r["Less Than 4 Weeks"] == 1: return "<4w"
            if r["Weeks 4-6"] == 1:         return "4-6w"
            if r["Greater Than 6 Weeks"]==1:return ">6w"
            return np.nan
        return df_.apply(pick, axis=1)
    # fallback: derive from Minimum Days to Surgery
    mds_col = find_col_by_names(df_.columns, MDS_CANDS)
    if mds_col is not None:
        v = pd.to_numeric(df_[mds_col], errors="coerce")
        def bins(x):
            try:
                w = float(x)/7.0
                if w < 4:  return "<4w"
                elif w<=6: return "4-6w"
                else:      return ">6w"
            except Exception:
                return np.nan
        return v.map(bins)
    return pd.Series([np.nan]*len(df_))
df["timing_bin"] = derive_timing(df)

ARM_LIST_NAT = [f"{r}@{t}" for r in ["FFX","GemCap","CRT"] for t in ["<4w","4-6w",">6w"]]
ARM2IDX_NAT = {a:i for i,a in enumerate(ARM_LIST_NAT)}

def nat_arm_idx(row):
    if row["S"] != 1: return -1
    r = row["regimen"]; t = row["timing_bin"]
    if pd.isna(r) or pd.isna(t): return -1
    return ARM2IDX_NAT.get(f"{r}@{t}", -1)

A_nat_idx = df.apply(nat_arm_idx, axis=1).astype(int).values
mask_nat = (df["S"] == 1).values
mask_us  = (df["S"] == 0).values

# composite complications (optional)
RES_Y = {}
for k, v in OUTCOME_CANDS.items():
    col = find_col_by_names(df.columns, v)
    if col is None: raise ValueError(f"Outcome column not found for: {v}")
    RES_Y[k] = col
df[COMP_NAME] = (
    to01(df[RES_Y["Fistula"]]) |
    to01(df[RES_Y["Infection"]]) |
    to01(df[RES_Y["Delayed Gastric Emptying"]]) |
    to01(df[RES_Y["Death 90 Days"]])
).astype(int)
y_comp = df[COMP_NAME].values.astype(int)

# -----------------------------------------------------------------------------
# OS data (time/event within 365d)
# -----------------------------------------------------------------------------
time_col  = find_col_by_names(df.columns, TIME_CANDS)
event_col = find_col_by_names(df.columns, EVENT_CANDS)
if (time_col is None) or (event_col is None):
    raise ValueError("Columns for OS not found: 'Survival Days' / 'Death 365 Days' (or aliases).")

t_days = pd.to_numeric(df[time_col], errors="coerce").fillna(0.0).values.astype(float)
d_1y   = to01(df[event_col]).astype(int)  # 1 = death within 365 days
tau = 365.0
observed_at_tau = ((t_days >= tau) | ((t_days < tau) & (d_1y==1))).astype(int)  # not explicitly used below

# -----------------------------------------------------------------------------
# covariate matrix X (exclude leakage/id/treatment indicators)
# -----------------------------------------------------------------------------
exclude_norm = {norm_name(x) for x in (USER_EXCLUDE | MORE_EXCLUDE)}
exclude_cols = set()
for c in df.columns:
    if is_id_like(c) or c in RES_Y.values() or c == COMP_NAME: exclude_cols.add(c); continue
    if norm_name(c) in exclude_norm: exclude_cols.add(c); continue
    if c in TIMING_BIN_COLS or norm_name(c) in {norm_name(x) for x in TIMING_BIN_COLS}: exclude_cols.add(c); continue
    if norm_name(c) in {norm_name(x) for x in MDS_CANDS}: exclude_cols.add(c); continue
    if c in ["regimen","timing_bin","S"]: exclude_cols.add(c); continue
    if c == nat_col: exclude_cols.add(c); continue
    if c in [time_col, event_col]: exclude_cols.add(c); continue

covar_cols = sorted([c for c in df.columns if c not in exclude_cols])
covar_cols = [c for c in covar_cols if norm_name(c) not in {norm_name("Mean Arterial Pressure"), norm_name("Hematocrit")}]

X_raw = df[covar_cols].copy()

# split types
num_cols, cat_cols = [], []
for c in X_raw.columns:
    s = X_raw[c]
    if pd.api.types.is_numeric_dtype(s):
        if (not np.issubdtype(s.dtype, np.floating)) and s.dropna().nunique() <= MAX_CAT_UNIQUE:
            cat_cols.append(c)
        else:
            num_cols.append(c)
    else:
        if s.astype(str).nunique() <= MAX_CAT_UNIQUE:
            cat_cols.append(c)

pre_X = build_preprocessor(num_cols, cat_cols)
X_enc = pre_X.fit_transform(X_raw)
Xdense = X_enc.toarray() if hasattr(X_enc, "toarray") else X_enc  # densify if needed

# -----------------------------------------------------------------------------
# selection model s(X)=P(S=1|X) → transport weights
# -----------------------------------------------------------------------------
sel_clf = LogisticRegression(max_iter=5000, solver="lbfgs", random_state=RANDOM_STATE)
sel_clf.fit(Xdense, df["S"].values)
s_hat = np.clip(sel_clf.predict_proba(Xdense)[:,1], PI_CLIP, 1-PI_CLIP)
pS = float(df["S"].mean())
w_sel = np.where(df["S"].values==1, pS/s_hat, (1-pS)/(1-s_hat))
w_sel = truncate_weights(w_sel, W_TRUNC_PCT)

# -----------------------------------------------------------------------------
# NAT arm propensity e_a(X) inside S=1
# -----------------------------------------------------------------------------
msk_nat_obs = mask_nat & (A_nat_idx>=0)
if msk_nat_obs.sum() < 50:
    raise ValueError("Too few known NAT-arm samples to estimate e_a(X) stably.")

prop_nat = LogisticRegression(solver="lbfgs", random_state=RANDOM_STATE, max_iter=5000, multi_class="auto")
prop_nat.fit(Xdense[msk_nat_obs], A_nat_idx[msk_nat_obs])
e_nat = np.zeros((len(df), 9)) + 1.0/9.0
e_nat[mask_nat] = np.clip(prop_nat.predict_proba(Xdense[mask_nat]), PI_CLIP, 1-PI_CLIP)
e_us = np.ones(len(df))  # for US path (S=0)

# -----------------------------------------------------------------------------
# core: OS@365 risk per arm — Cox PH preferred, fallback IPCW-Logit
# -----------------------------------------------------------------------------
def km_censor_weights(times, status, tau_):
    """
    Simple IPCW using marginal Kaplan-Meier for censoring:
      times  : observed time (days)
      status : 1 = death event by 365d, 0 = no death (censored)
    Returns G_hat(min(T_i, tau)) and weights 1/G_hat(...)
    """
    t_trunc = np.minimum(times, tau_)
    is_censor_event = (status==0) & (times <= tau_)
    uniq = np.sort(np.unique(t_trunc[is_censor_event]))
    n = len(times)
    G = np.ones(n, dtype=float)
    for tj in uniq:
        d_j = np.sum(is_censor_event & (t_trunc==tj))
        n_j = np.sum(t_trunc >= tj)
        step = (1.0 - d_j / max(n_j,1))
        G[t_trunc >= tj] *= step
    G = np.clip(G, 1e-6, 1.0)
    W = 1.0 / G
    return G, W

Ghat, W_ipcw = km_censor_weights(t_days, d_1y, tau)

# lifelines availability
use_lifelines = True
try:
    from lifelines import CoxPHFitter
except Exception:
    use_lifelines = False

OS_RISK = np.zeros((len(df), 10))  # 10 arms: US + 9 NAT arms
ARM_NAMES = ["US"] + [f"{r}@{t}" for r in ["FFX","GemCap","CRT"] for t in ["<4w","4-6w",">6w"]]

def df_for_arm(indices):
    d = pd.DataFrame(X_raw.iloc[indices].copy())
    d["_time_"]  = t_days[indices]
    d["_event_"] = d_1y[indices]
    return d

# US arm (fit within S=0)
if mask_us.sum() < 30:
    raise ValueError("Too few S=0 (US) samples to fit US OS model.")
idx_us = np.where(mask_us)[0]

if use_lifelines:
    try:
        df_us = df_for_arm(idx_us)
        cph_us = CoxPHFitter(penalizer=0.1)
        cph_us.fit(df_us, duration_col="_time_", event_col="_event_", show_progress=False)
        surv_us = np.clip(
            cph_us.predict_survival_function(df_for_arm(np.arange(len(df))), times=[tau]).T.values[:,0],
            1e-6, 1.0
        )
        OS_RISK[:,0] = 1.0 - surv_us
    except Exception as e:
        use_lifelines = False
        print("lifelines Cox fit failed on US arm; falling back to IPCW-Logit. Error:", e)

if not use_lifelines:
    logit_us = LogisticRegression(max_iter=5000, solver="lbfgs", random_state=RANDOM_STATE)
    logit_us.fit(Xdense[idx_us], d_1y[idx_us], sample_weight=W_ipcw[idx_us])
    OS_RISK[:,0] = np.clip(logit_us.predict_proba(Xdense)[:,1], 1e-6, 1-1e-6)

# NAT arms
for j, arm in enumerate(ARM_NAMES[1:], start=1):
    ridx = ARM2IDX_NAT[arm]
    idx_arm = np.where((A_nat_idx==ridx) & mask_nat)[0]
    if len(idx_arm) < 30:
        # fallback: IPCW-Logit (either pooled S=1 or arm-specific reduced)
        logit = LogisticRegression(max_iter=5000, solver="lbfgs", random_state=RANDOM_STATE)
        logit.fit(Xdense[mask_nat], d_1y[mask_nat], sample_weight=W_ipcw[mask_nat])
        OS_RISK[:,j] = np.clip(logit.predict_proba(Xdense)[:,1], 1e-6, 1-1e-6)
        continue
    if use_lifelines:
        try:
            df_arm = df_for_arm(idx_arm)
            cph = CoxPHFitter(penalizer=0.1)
            cph.fit(df_arm, duration_col="_time_", event_col="_event_", show_progress=False)
            surv = np.clip(
                cph.predict_survival_function(df_for_arm(np.arange(len(df))), times=[tau]).T.values[:,0],
                1e-6, 1.0
            )
            OS_RISK[:,j] = 1.0 - surv
            continue
        except Exception as e:
            print(f"lifelines Cox fit failed on arm {arm}; using IPCW-Logit. Error:", e)
    logit = LogisticRegression(max_iter=5000, solver="lbfgs", random_state=RANDOM_STATE)
    logit.fit(Xdense[idx_arm], d_1y[idx_arm], sample_weight=W_ipcw[idx_arm])
    OS_RISK[:,j] = np.clip(logit.predict_proba(Xdense)[:,1], 1e-6, 1-1e-6)

# export OS risk matrix
osrisk_df = pd.DataFrame(OS_RISK, columns=ARM_NAMES)
osrisk_df.to_csv(os.path.join(OUTROOT, "tables/OS_risk_matrix_365d.csv"), index=False)

# -----------------------------------------------------------------------------
# plug OS into transport AIPW policy evaluation
# -----------------------------------------------------------------------------
USE_COMPOSITE = False  # OS only by default (set True to blend)
W_OS, W_COMP = 1.0, 0.0  # e.g., 0.6/0.4 when blending

# complications models (simple GBDT as placeholder)
clf_us_comp = GradientBoostingClassifier(n_estimators=300, learning_rate=0.05, max_depth=3,
                                         subsample=0.8, random_state=RANDOM_STATE)
clf_us_comp.fit(Xdense[mask_us], y_comp[mask_us])
comp_us_pred = np.clip(clf_us_comp.predict_proba(Xdense)[:,1], 1e-6, 1-1e-6)

comp_nat_pred = np.zeros((len(df), 9))
for s in range(9):
    msk = (mask_nat) & (A_nat_idx==s)
    if msk.sum() < 30:
        clf = GradientBoostingClassifier(n_estimators=300, learning_rate=0.05, max_depth=3,
                                         subsample=0.8, random_state=RANDOM_STATE)
        clf.fit(Xdense[mask_nat], y_comp[mask_nat])
        comp_nat_pred[:,s] = np.clip(clf.predict_proba(Xdense)[:,1], 1e-6, 1-1e-6)
    else:
        clf = GradientBoostingClassifier(n_estimators=300, learning_rate=0.05, max_depth=3,
                                         subsample=0.8, random_state=RANDOM_STATE)
        clf.fit(Xdense[msk], y_comp[msk])
        comp_nat_pred[:,s] = np.clip(clf.predict_proba(Xdense)[:,1], 1e-6, 1-1e-6)

# LOSS = W_OS * OS + W_COMP * composite
LOSS_US  = W_OS*OS_RISK[:,0] + W_COMP*comp_us_pred
LOSS_NAT = W_OS*OS_RISK[:,1:] + W_COMP*comp_nat_pred

# Stage-2: best NAT arm per individual
best_nat_arm = np.argmin(LOSS_NAT, axis=1)

# Stage-1: compare NAT vs US under tau margin
delta_nat_vs_us = LOSS_NAT[np.arange(len(df)), best_nat_arm] - LOSS_US  # negative → NAT better

def build_policy(tau_):
    rec = np.full(len(df), -1, dtype=int)  # -1 = US; 0..8 = NAT arm index
    choose_nat = (delta_nat_vs_us < -tau_)
    rec[choose_nat] = best_nat_arm[choose_nat]
    return rec

def policy_value_transport(rec):
    """
    g-transport AIPW over LOSS (event probability as 'outcome'):
      - model term m(X) from risk models above
      - IPW correction only for arms actually observed
    """
    numer, denom = 0.0, 0.0
    for i in range(len(df)):
        wi = w_sel[i]
        if rec[i] == -1:
            m = LOSS_US[i]
            a_obs = int(mask_us[i] and (df["S"].values[i]==0))
            numer += wi * (m + a_obs * ((y_comp[i] if USE_COMPOSITE else d_1y[i]) - m) / e_us[i])
            denom += wi
        else:
            s = rec[i]
            m = LOSS_NAT[i, s]
            a_obs = int((A_nat_idx[i]==s) and (df["S"].values[i]==1))
            numer += wi * (m + a_obs * ((y_comp[i] if USE_COMPOSITE else d_1y[i]) - m) / e_nat[i, s])
            denom += wi
    return numer / max(denom, 1e-12)

# tau search
best_val, best_tau = 1e9, None
for tau_ in TAU_GRID:
    val = policy_value_transport(build_policy(tau_))
    if val < best_val:
        best_val, best_tau = val, tau_

rec_final = build_policy(best_tau)

# baselines
us_only = np.full(len(df), -1, dtype=int)
best_nat_arm_global = int(np.argmin(np.mean(LOSS_NAT, axis=0)))
fixed_best_nat = np.full(len(df), best_nat_arm_global, dtype=int)

def bootstrap_ci(rec, B=BOOT_B, seed=RANDOM_STATE):
    rng = np.random.RandomState(seed); vals=[]
    for _ in range(B):
        idx = rng.choice(len(df), len(df), replace=True)
        vals.append(policy_value_transport(rec[idx]))
    vals = np.array(vals)
    return float(vals.mean()), float(np.quantile(vals,0.025)), float(np.quantile(vals,0.975))

mean_pol, lo_pol, hi_pol = bootstrap_ci(rec_final)
mean_us , lo_us , hi_us  = bootstrap_ci(us_only)
mean_fix, lo_fix, hi_fix = bootstrap_ci(fixed_best_nat)

# results table + plot
RES = pd.DataFrame({
    "Policy": [
        f"DRIFT-NAT (OS{' + '+COMP_NAME if USE_COMPOSITE else ''})",
        "US only",
        "Fixed best NAT arm"
    ],
    "Transport_Value": [mean_pol, mean_us, mean_fix],
    "CI_lo": [lo_pol, lo_us, hi_fix if False else lo_fix],  # keep order (pol, us, fix)
    "CI_hi": [hi_pol, hi_us, hi_fix]
})
RES.to_csv(os.path.join(OUTROOT, "tables/policy_value_OS.csv"), index=False)

fig = plt.figure(figsize=(9,6))
ax = fig.add_subplot(111)
ax.bar(
    RES["Policy"], RES["Transport_Value"],
    yerr=[RES["Transport_Value"]-RES["CI_lo"], RES["CI_hi"]-RES["Transport_Value"]],
    capsize=4
)
plt.xticks(rotation=18, ha="right")
ax.set_ylabel(f"Event probability at 365d ({'OS only' if not USE_COMPOSITE else 'OS + composite'})")
ax.set_title("Policy value on target population (±95% CI)")
savefig(fig, os.path.join(OUTROOT, "figs/policy_value_OS.png"))

# patient-level export
out_pat = pd.DataFrame({
    "LOSS_US": LOSS_US,
    **{f"LOSS_{ARM_NAMES[j]}": LOSS_NAT[:,j-1] for j in range(1,10)},
    "rec_final": [ "US" if v==-1 else ARM_LIST_NAT[v] for v in rec_final ],
    "OS_risk_US": OS_RISK[:,0],
    **{f"OS_risk_{ARM_NAMES[j]}": OS_RISK[:,j] for j in range(1,10)}
})
out_pat.to_csv(os.path.join(OUTROOT, "tables/patient_level_OS_and_recommendation.csv"), index=False)

with open(os.path.join(OUTROOT,"meta_OS.json"), "w", encoding="utf-8") as f:
    json.dump({
        "model_name": "DRIFT-NAT + OS",
        "tau_selected": float(best_tau),
        "use_lifelines": bool(use_lifelines),
        "use_composite": bool(USE_COMPOSITE),
        "arms": ARM_NAMES,
        "root_dir": OUTROOT
    }, f, ensure_ascii=False, indent=2)

print("\n[DONE] Step9-OS outputs at:", OUTROOT)