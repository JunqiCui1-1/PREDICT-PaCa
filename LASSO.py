# -*- coding: utf-8 -*-
"""Untitled77.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tmvaBxkpCK70PzUHQ0h53_KL7V9CXe51
"""

#!/usr/bin/env python3
# =============================================================================
# Component X — Step 2 · LASSO (Logistic L1) feature selection + publication-grade plots
#
# GITHUB-STYLE HEADER
# -----------------------------------------------------------------------------
# DESCRIPTION
#   Perform L1-regularized logistic regression (LASSO) for feature selection,
#   plot coefficient paths, stability selection, model performance (ROC/PR),
#   calibration, confusion matrices, and export key CSV artifacts. All plots
#   are single-image files (no subplots), rendered at 400 dpi without forcing
#   specific colors to keep a clean, reproducible GitHub style.
#
# INPUTS (CSV; relative to repo root by default, override via env vars)
#   - ./data/Chort_train.csv
#   - ./data/Chort_valid.csv
#   - (optional) ./data/Chort_total.csv
#
# OUTPUTS (created under ./outputs/step2_lasso_<timestamp>/)
#   - 00_feature_types.csv
#   - 01_missingness_*_top30.png
#   - 02_outcome_rate_*.png
#   - 10_lasso_paths_metrics.csv
#   - 11_coef_paths.png
#   - 12_selected_vs_C.png
#   - 13_auc_vs_C.png
#   - 14_ap_vs_C.png
#   - 20_best_model_summary.json
#   - 21_best_model_coef.csv
#   - 22_top_positive_coef.png
#   - 23_top_negative_coef.png
#   - 24_coef_hist_best.png
#   - 30_roc_curves.png
#   - 31_pr_curves.png
#   - 32_pred_hist_train.png
#   - 32_pred_hist_valid.png
#   - 33_calibration_train.png
#   - 33_calibration_valid.png
#   - 34_cm_train.png
#   - 34_cm_valid.png
#   - 40_stability_selection.csv
#   - 41_stability_top.png
#   - 42_stability_score_top.png
#   - 50_pred_train.csv
#   - 51_pred_valid.csv
#   - 60_summary_metrics.json
#   - 61_selected_features_bestC.csv
#   - step2_lasso_outputs_<timestamp>.zip
#
# REQUIREMENTS (add to requirements.txt)
#   pandas, numpy, scikit-learn, matplotlib
#
# USAGE
#   python path/to/this_script.py
#   # Optional environment overrides:
#   #   PREDICT_PACA_TRAIN   (default: ./data/Chort_train.csv)
#   #   PREDICT_PACA_VALID   (default: ./data/Chort_valid.csv)
#   #   PREDICT_PACA_TOTAL   (default: ./data/Chort_total.csv)
#   #   PREDICT_PACA_OUTPUTS (default: ./outputs)
# =============================================================================

import os
import json
import math
import time
import zipfile
from datetime import datetime
from typing import List, Tuple, Dict, Optional

import numpy as np
import pandas as pd
import matplotlib
import matplotlib.pyplot as plt

from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import (
    roc_auc_score, average_precision_score,
    RocCurveDisplay, PrecisionRecallDisplay,
    confusion_matrix, ConfusionMatrixDisplay,
    brier_score_loss
)
from sklearn.calibration import calibration_curve

# -----------------------------------------------------------------------------
# global config (GitHub-style paths & timestamped output dir)
# -----------------------------------------------------------------------------
TRAIN_CSV = os.environ.get("PREDICT_PACA_TRAIN", "./data/Chort_train.csv")
VALID_CSV = os.environ.get("PREDICT_PACA_VALID", "./data/Chort_valid.csv")
TOTAL_CSV = os.environ.get("PREDICT_PACA_TOTAL", "./data/Chort_total.csv")  # optional
OUT_ROOT = os.environ.get("PREDICT_PACA_OUTPUTS", "./outputs")

TS = datetime.now().strftime("%Y%m%d_%H%M%S")
OUT_DIR = os.path.join(OUT_ROOT, f"step2_lasso_{TS}")
os.makedirs(OUT_DIR, exist_ok=True)
MANIFEST: List[str] = []  # record all produced files

# outcome column (binary 0/1)
AUTO_DETECT_OUTCOME = False
OUTCOME_NAME = "Death 365 Days"  # set to your actual column name if not using auto-detect

# id-like columns to exclude from features
ID_KEYS = {"id", "subject_id", "patient_id", "mrn", "patientunitstayid"}

# high-cardinality threshold for categorical detection (above → treat as numeric/ignore OHE)
MAX_CAT_UNIQUE = 30

# L1 Logistic search grid (more points → finer but slower)
CS_GRID = np.logspace(-3, 2, 20)  # 0.001 … 100, 20 points
RANDOM_STATE = 42

# stability selection via bootstrap
STABILITY_BOOTSTRAPS = 50
STABILITY_SAMPLE_FRAC = 0.8
N_TOP_COEF_TO_PLOT = 30

# -----------------------------------------------------------------------------
# small utils
# -----------------------------------------------------------------------------
def savefig(fig, name: str) -> str:
    """Save a 400-dpi PNG and close the figure."""
    path = os.path.join(OUT_DIR, f"{name}.png")
    fig.savefig(path, dpi=400, bbox_inches="tight")
    plt.close(fig)
    MANIFEST.append(path)
    return path

def dump_df(df: pd.DataFrame, name: str) -> str:
    path = os.path.join(OUT_DIR, f"{name}.csv")
    df.to_csv(path, index=False)
    MANIFEST.append(path)
    return path

def read_csv_any(path: str) -> pd.DataFrame:
    """Robust CSV reader with a few common encodings."""
    for enc in ("utf-8", "utf-8-sig", "latin1"):
        try:
            return pd.read_csv(path, encoding=enc, low_memory=False)
        except Exception:
            continue
    return pd.read_csv(path, low_memory=False)

def guess_outcome_name(cols: List[str]) -> Optional[str]:
    candidates = {
        "os_1y", "os1y", "os_12m", "one_year_os",
        "death_1y", "death1y", "status", "os"
    }
    for c in cols:
        if c.lower() in candidates:
            return c
    return None

def is_id_like(col: str) -> bool:
    lc = col.lower()
    return (lc in ID_KEYS) or lc.endswith("_id")

def split_feature_types(df: pd.DataFrame, outcome_col: str) -> Tuple[List[str], List[str]]:
    """Split into numeric vs. categorical features based on dtype and cardinality."""
    num_cols, cat_cols = [], []
    for c in df.columns:
        if c == outcome_col or is_id_like(c):
            continue
        s = df[c]
        if pd.api.types.is_numeric_dtype(s):
            if s.dropna().nunique() <= MAX_CAT_UNIQUE and not np.issubdtype(s.dtype, np.floating):
                cat_cols.append(c)
            else:
                num_cols.append(c)
        else:
            if s.astype(str).nunique() <= MAX_CAT_UNIQUE:
                cat_cols.append(c)
            # else: skip ultra high-cardinality object column
    return num_cols, cat_cols

def _ohe_has_sparse_output() -> bool:
    return "sparse_output" in OneHotEncoder.__init__.__code__.co_varnames

def build_preprocessor(num_cols: List[str], cat_cols: List[str]) -> ColumnTransformer:
    """Numeric: median-impute + StandardScaler(with_mean=False).
       Categorical: most-frequent-impute + OneHotEncoder(handle_unknown='ignore')."""
    num_pipe = Pipeline(steps=[
        ("imputer", SimpleImputer(strategy="median")),
        ("scaler", StandardScaler(with_mean=False))
    ])
    if _ohe_has_sparse_output():
        ohe = OneHotEncoder(handle_unknown="ignore", sparse_output=True)
    else:
        ohe = OneHotEncoder(handle_unknown="ignore", sparse=True)

    cat_pipe = Pipeline(steps=[
        ("imputer", SimpleImputer(strategy="most_frequent")),
        ("onehot", ohe)
    ])
    pre = ColumnTransformer(
        transformers=[
            ("num", num_pipe, num_cols),
            ("cat", cat_pipe, cat_cols),
        ],
        remainder="drop",
        sparse_threshold=1.0,
        verbose_feature_names_out=False
    )
    return pre

def get_feature_names(preprocessor: ColumnTransformer) -> List[str]:
    """Extract final feature names after ColumnTransformer."""
    out: List[str] = []
    for name, trans, cols in preprocessor.transformers_:
        if name == "remainder" and trans == "drop":
            continue
        if hasattr(trans, "named_steps"):
            # numeric branch
            if "scaler" in trans.named_steps:
                out.extend(list(cols))
            # categorical branch
            if "onehot" in trans.named_steps:
                ohe = trans.named_steps["onehot"]
                if hasattr(ohe, "get_feature_names_out"):
                    ohe_names = list(ohe.get_feature_names_out(cols))
                else:
                    ohe_names = list(ohe.get_feature_names(cols))
                out.extend(ohe_names)
        else:
            # bare transformers (rare in this script)
            out.extend(list(cols))
    return out

# -----------------------------------------------------------------------------
# load data
# -----------------------------------------------------------------------------
df_train = read_csv_any(TRAIN_CSV)
df_valid = read_csv_any(VALID_CSV)
df_total = read_csv_any(TOTAL_CSV) if os.path.exists(TOTAL_CSV) else None

# outcome column
if AUTO_DETECT_OUTCOME:
    OUTCOME_NAME = guess_outcome_name(list(df_train.columns)) or OUTCOME_NAME
if OUTCOME_NAME not in df_train.columns:
    raise ValueError(f"Outcome column '{OUTCOME_NAME}' not found in training CSV.")

# simple EDA: missingness (Top 30) and outcome rate
def plot_missingness(df: pd.DataFrame, title: str, fname: str, topk: int = 30):
    na = df.isna().mean().sort_values(ascending=False).head(topk)
    fig = plt.figure(figsize=(8, 6))
    ax = fig.add_subplot(111)
    ax.barh(na.index.astype(str), na.values)
    ax.set_title(title)
    ax.set_xlabel("Proportion Missing")
    plt.tight_layout()
    savefig(fig, fname)

plot_missingness(df_train, "Missingness (Top 30) — Train", "01_missingness_train_top30")
plot_missingness(df_valid, "Missingness (Top 30) — Valid", "01_missingness_valid_top30")
if df_total is not None:
    plot_missingness(df_total, "Missingness (Top 30) — Total", "01_missingness_total_top30")

def plot_outcome_rate(df: pd.DataFrame, split: str):
    if OUTCOME_NAME in df.columns:
        y = pd.Series(df[OUTCOME_NAME]).dropna()
        rate = float(np.mean(y))
        fig = plt.figure()
        ax = fig.add_subplot(111)
        ax.bar([split], [rate])
        ax.set_ylim(0, 1)
        ax.set_title(f"{OUTCOME_NAME} rate — {split}")
        ax.set_ylabel("Proportion")
        savefig(fig, f"02_outcome_rate_{split.lower()}")

plot_outcome_rate(df_train, "Train")
plot_outcome_rate(df_valid, "Valid")
if df_total is not None:
    plot_outcome_rate(df_total, "Total")

# -----------------------------------------------------------------------------
# align columns between train/valid and build features
# -----------------------------------------------------------------------------
common_cols = sorted(list(set(df_train.columns) & set(df_valid.columns)))
if OUTCOME_NAME not in common_cols:
    raise ValueError("Validation CSV does not contain the outcome column; please align schemas.")

df_train = df_train[common_cols].copy()
df_valid = df_valid[common_cols].copy()

num_cols, cat_cols = split_feature_types(df_train.drop(columns=[OUTCOME_NAME]), OUTCOME_NAME)
meta_info = pd.DataFrame({
    "feature": num_cols + cat_cols,
    "type": (["numeric"] * len(num_cols)) + (["categorical"] * len(cat_cols))
})
dump_df(meta_info, "00_feature_types")

X_train = df_train.drop(columns=[OUTCOME_NAME])
y_train = df_train[OUTCOME_NAME].astype(int).values
X_valid = df_valid.drop(columns=[OUTCOME_NAME])
y_valid = df_valid[OUTCOME_NAME].astype(int).values

preprocessor = build_preprocessor(num_cols, cat_cols)
X_train_trans = preprocessor.fit_transform(X_train)
X_valid_trans = preprocessor.transform(X_valid)
feat_names = get_feature_names(preprocessor)

# -----------------------------------------------------------------------------
# LASSO grid over C values: coefficient paths & performance curves
# -----------------------------------------------------------------------------
coef_path = []
n_selected_path = []
auc_train_path, auc_valid_path = [], []
ap_train_path, ap_valid_path = [], []

best_auc_valid = -np.inf
best_model: Optional[LogisticRegression] = None
best_C: Optional[float] = None
best_coef: Optional[np.ndarray] = None

for C in CS_GRID:
    clf = LogisticRegression(
        penalty="l1",
        C=C,
        solver="liblinear",
        random_state=RANDOM_STATE,
        max_iter=2000
    )
    clf.fit(X_train_trans, y_train)
    coef = clf.coef_.ravel()
    coef_path.append(coef)
    n_selected_path.append(int(np.sum(coef != 0)))

    p_train = clf.predict_proba(X_train_trans)[:, 1]
    p_valid = clf.predict_proba(X_valid_trans)[:, 1]

    auc_tr = roc_auc_score(y_train, p_train)
    auc_va = roc_auc_score(y_valid, p_valid)
    ap_tr  = average_precision_score(y_train, p_train)
    ap_va  = average_precision_score(y_valid, p_valid)

    auc_train_path.append(auc_tr); auc_valid_path.append(auc_va)
    ap_train_path.append(ap_tr);   ap_valid_path.append(ap_va)

    if auc_va > best_auc_valid:
        best_auc_valid = auc_va
        best_model = clf
        best_C = C
        best_coef = coef

coef_path_arr = np.vstack(coef_path)  # [len(CS_GRID), n_features]
path_df = pd.DataFrame(coef_path_arr, columns=feat_names)
path_df.insert(0, "C", CS_GRID)
path_df.insert(1, "n_selected", n_selected_path)
path_df.insert(2, "AUC_train", auc_train_path)
path_df.insert(3, "AUC_valid", auc_valid_path)
path_df.insert(4, "AP_train", ap_train_path)
path_df.insert(5, "AP_valid", ap_valid_path)
dump_df(path_df, "10_lasso_paths_metrics")

# coefficient paths vs log10(C)
fig = plt.figure(figsize=(10, 7))
ax = fig.add_subplot(111)
x = np.log10(CS_GRID)
for j in range(coef_path_arr.shape[1]):
    ax.plot(x, coef_path_arr[:, j], linewidth=0.8)
ax.set_xlabel("log10(C)")
ax.set_ylabel("Coefficient")
ax.set_title("LASSO Coefficient Paths")
savefig(fig, "11_coef_paths")

# number of selected features vs C
fig = plt.figure()
ax = fig.add_subplot(111)
ax.plot(np.log10(CS_GRID), n_selected_path, marker="o")
ax.set_xlabel("log10(C)")
ax.set_ylabel("#Selected features (|β|>0)")
ax.set_title("Selected Features vs C")
savefig(fig, "12_selected_vs_C")

# AUC vs C
fig = plt.figure()
ax = fig.add_subplot(111)
ax.plot(np.log10(CS_GRID), auc_train_path, label="AUC Train")
ax.plot(np.log10(CS_GRID), auc_valid_path, label="AUC Valid")
ax.set_xlabel("log10(C)"); ax.set_ylabel("AUC"); ax.legend()
ax.set_title("AUC vs C")
savefig(fig, "13_auc_vs_C")

# AP vs C
fig = plt.figure()
ax = fig.add_subplot(111)
ax.plot(np.log10(CS_GRID), ap_train_path, label="AP Train")
ax.plot(np.log10(CS_GRID), ap_valid_path, label="AP Valid")
ax.set_xlabel("log10(C)"); ax.set_ylabel("Average Precision"); ax.legend()
ax.set_title("AP vs C")
savefig(fig, "14_ap_vs_C")

# best model summary & coefficients
best_summary = {
    "best_C": float(best_C),
    "best_auc_valid": float(best_auc_valid),
    "n_selected_best": int(np.sum(best_coef != 0))
}
with open(os.path.join(OUT_DIR, "20_best_model_summary.json"), "w", encoding="utf-8") as f:
    json.dump(best_summary, f, ensure_ascii=False, indent=2)
MANIFEST.append(os.path.join(OUT_DIR, "20_best_model_summary.json"))

coef_df = pd.DataFrame({"feature": feat_names, "coef": best_coef}).sort_values("coef", ascending=False)
dump_df(coef_df, "21_best_model_coef")

# top positive / negative coefficients
top_pos = coef_df.head(N_TOP_COEF_TO_PLOT)
top_neg = coef_df.sort_values("coef", ascending=True).head(N_TOP_COEF_TO_PLOT)

fig = plt.figure(figsize=(8, 8))
ax = fig.add_subplot(111)
ax.barh(top_pos["feature"][::-1], top_pos["coef"][::-1])
ax.set_title("Top Positive Coefficients (Best C)")
ax.set_xlabel("Coefficient")
plt.tight_layout()
savefig(fig, "22_top_positive_coef")

fig = plt.figure(figsize=(8, 8))
ax = fig.add_subplot(111)
ax.barh(top_neg["feature"][::-1], top_neg["coef"][::-1])
ax.set_title("Top Negative Coefficients (Best C)")
ax.set_xlabel("Coefficient")
plt.tight_layout()
savefig(fig, "23_top_negative_coef")

# coefficient histogram
fig = plt.figure()
ax = fig.add_subplot(111)
ax.hist(best_coef, bins=40)
ax.set_title("Coefficient Histogram (Best C)")
ax.set_xlabel("Coefficient"); ax.set_ylabel("Count")
savefig(fig, "24_coef_hist_best")

# -----------------------------------------------------------------------------
# performance & calibration (best C)
# -----------------------------------------------------------------------------
p_train = best_model.predict_proba(X_train_trans)[:, 1]
p_valid = best_model.predict_proba(X_valid_trans)[:, 1]

# ROC
fig = plt.figure()
ax = fig.add_subplot(111)
RocCurveDisplay.from_predictions(y_train, p_train, name="Train", ax=ax)
RocCurveDisplay.from_predictions(y_valid, p_valid, name="Valid", ax=ax)
ax.set_title("ROC Curves (Best C)")
savefig(fig, "30_roc_curves")

# PR
fig = plt.figure()
ax = fig.add_subplot(111)
PrecisionRecallDisplay.from_predictions(y_train, p_train, name="Train", ax=ax)
PrecisionRecallDisplay.from_predictions(y_valid, p_valid, name="Valid", ax=ax)
ax.set_title("Precision–Recall Curves (Best C)")
savefig(fig, "31_pr_curves")

# predicted probability hist by class
def plot_pred_hist(p, y, split_name):
    fig = plt.figure()
    ax = fig.add_subplot(111)
    ax.hist(p[y==0], bins=30, alpha=0.7, label="y=0")
    ax.hist(p[y==1], bins=30, alpha=0.7, label="y=1")
    ax.set_title(f"Predicted Probability by Class — {split_name}")
    ax.set_xlabel("Predicted probability")
    ax.set_ylabel("Count")
    ax.legend()
    savefig(fig, f"32_pred_hist_{split_name.lower()}")

plot_pred_hist(p_train, y_train, "Train")
plot_pred_hist(p_valid, y_valid, "Valid")

# calibration curves
def plot_calibration(p, y, split_name):
    prob_true, prob_pred = calibration_curve(y, p, n_bins=10, strategy="quantile")
    fig = plt.figure()
    ax = fig.add_subplot(111)
    ax.plot(prob_pred, prob_true, marker="o")
    ax.plot([0,1],[0,1],"--")
    ax.set_title(f"Calibration Curve — {split_name}")
    ax.set_xlabel("Mean predicted probability")
    ax.set_ylabel("Fraction of positives")
    savefig(fig, f"33_calibration_{split_name.lower()}")

plot_calibration(p_train, y_train, "Train")
plot_calibration(p_valid, y_valid, "Valid")

# confusion matrices at threshold 0.5
def plot_cm(p, y, split_name, thr=0.5):
    yhat = (p >= thr).astype(int)
    cm = confusion_matrix(y, yhat)
    fig = plt.figure()
    ax = fig.add_subplot(111)
    disp = ConfusionMatrixDisplay(confusion_matrix=cm)
    disp.plot(values_format="d", ax=ax, colorbar=False)
    ax.set_title(f"Confusion Matrix ({split_name}, thr={thr:.2f})")
    savefig(fig, f"34_cm_{split_name.lower()}")

plot_cm(p_train, y_train, "Train")
plot_cm(p_valid, y_valid, "Valid")

# -----------------------------------------------------------------------------
# stability selection (bootstrap frequency at best C)
# -----------------------------------------------------------------------------
rng = np.random.RandomState(RANDOM_STATE)
sel_counts = np.zeros_like(best_coef, dtype=int)

for b in range(STABILITY_BOOTSTRAPS):
    idx = rng.choice(len(y_train), size=int(STABILITY_SAMPLE_FRAC * len(y_train)), replace=True)
    Xb = X_train_trans[idx]
    yb = y_train[idx]
    clf = LogisticRegression(
        penalty="l1", C=best_C, solver="liblinear",
        random_state=RANDOM_STATE + b, max_iter=2000
    )
    clf.fit(Xb, yb)
    sel_counts += (clf.coef_.ravel() != 0).astype(int)

sel_freq = sel_counts / float(STABILITY_BOOTSTRAPS)
stability_df = pd.DataFrame({"feature": feat_names, "sel_freq": sel_freq, "coef_bestC": best_coef})
stability_df = stability_df.sort_values(["sel_freq", "coef_bestC"], ascending=[False, False])
dump_df(stability_df, "40_stability_selection")

# stability top (by frequency)
fig = plt.figure(figsize=(8, 8))
ax = fig.add_subplot(111)
top_stab = stability_df.head(N_TOP_COEF_TO_PLOT).sort_values("sel_freq", ascending=True)
ax.barh(top_stab["feature"], top_stab["sel_freq"])
ax.set_title("Stability Selection Frequency (Top)")
ax.set_xlabel("Selection frequency")
plt.tight_layout()
savefig(fig, "41_stability_top")

# stability score = |coef| × freq (top)
fig = plt.figure(figsize=(8, 8))
ax = fig.add_subplot(111)
score = np.abs(stability_df["coef_bestC"]) * stability_df["sel_freq"]
top_score = stability_df.assign(score=score).sort_values("score", ascending=False).head(N_TOP_COEF_TO_PLOT)
ax.barh(top_score["feature"][::-1], top_score["score"][::-1])
ax.set_title("Stability Score = |coef| × freq (Top)")
ax.set_xlabel("Score")
plt.tight_layout()
savefig(fig, "42_stability_score_top")

# -----------------------------------------------------------------------------
# predictions & summary exports
# -----------------------------------------------------------------------------
pred_df_train = pd.DataFrame({"y": y_train, "p": p_train})
pred_df_valid = pd.DataFrame({"y": y_valid, "p": p_valid})
dump_df(pred_df_train, "50_pred_train")
dump_df(pred_df_valid, "51_pred_valid")

summary_metrics = {
    "best_C": float(best_C),
    "train_auc": float(roc_auc_score(y_train, p_train)),
    "valid_auc": float(roc_auc_score(y_valid, p_valid)),
    "train_ap": float(average_precision_score(y_train, p_train)),
    "valid_ap": float(average_precision_score(y_valid, p_valid)),
    "train_brier": float(brier_score_loss(y_train, p_train)),
    "valid_brier": float(brier_score_loss(y_valid, p_valid)),
    "n_selected_best": int(np.sum(best_coef != 0)),
    "n_features_after_encode": int(len(best_coef)),
    "n_numeric_raw": int(len(num_cols)),
    "n_categorical_raw": int(len(cat_cols)),
}
with open(os.path.join(OUT_DIR, "60_summary_metrics.json"), "w", encoding="utf-8") as f:
    json.dump(summary_metrics, f, ensure_ascii=False, indent=2)
MANIFEST.append(os.path.join(OUT_DIR, "60_summary_metrics.json"))

# non-zero features at best C
selected_df = coef_df.loc[coef_df["coef"] != 0].copy()
dump_df(selected_df, "61_selected_features_bestC")

# -----------------------------------------------------------------------------
# zip everything
# -----------------------------------------------------------------------------
zip_path = os.path.join(OUT_DIR, f"step2_lasso_outputs_{TS}.zip")
with zipfile.ZipFile(zip_path, "w", zipfile.ZIP_DEFLATED) as zf:
    for f in MANIFEST:
        if os.path.isfile(f):
            zf.write(f, arcname=os.path.basename(f))

print(f"[DONE] All figures & files saved to: {OUT_DIR}")
print(f"[ZIP ] {zip_path}")