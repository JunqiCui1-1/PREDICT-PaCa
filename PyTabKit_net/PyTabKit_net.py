# -*- coding: utf-8 -*-
"""Untitled77.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tmvaBxkpCK70PzUHQ0h53_KL7V9CXe51
"""

#!/usr/bin/env python3
# =============================================================================
# Component 1 â€” PyTabKit-Net (Improved Core Model)
# End-to-end, robust training & evaluation for preoperative risk prediction.
#
# GITHUB-STYLE HEADER
# -----------------------------------------------------------------------------
# DESCRIPTION
#   Calibration-first tabular learner with teacher-ensemble distillation,
#   sparse stacking, soft-voting with calibration, and feasibility-friendly
#   outputs (ROC, calibration curves, confusion matrices, metrics tables).
#
# INPUTS (CSV; relative to repo root)
#   - ./data/Chort_train.csv
#   - ./data/Chort_valid.csv
#
# OUTPUTS (created under ./outputs/)
#   - params_table.csv
#   - metrics_table.csv     # includes @0.5 / @bestF1 / @R@P>=0.55 / @Youden
#   - roc_all.png
#   - calibration_all.png
#   - confusion_<MODEL>.png # unified navy palette, 400 dpi
#   - ensemble_weights.json
#
# KEY DESIGN CHOICES
#   - Include CatBoost so all six models are available for plots.
#   - Unify primary model display name as "PyTabKit-Net".
#   - Draw ROC/Calibration only for the specified six models.
#   - Safe torch import; if unavailable, fall back to sklearn MLP backbone.
#
# REQUIREMENTS (add to requirements.txt)
#   pandas, numpy, scikit-learn, matplotlib, xgboost, lightgbm, catboost,
#   scipy
#
# USAGE
#   python path/to/this_script.py
#   # Optional: override paths with environment variables below.
#
# ENV OVERRIDES
#   PREDICT_PACA_TRAIN   # default: ./data/Chort_train.csv
#   PREDICT_PACA_VALID   # default: ./data/Chort_valid.csv
#   PREDICT_PACA_OUTPUTS # default: ./outputs
#
# REPRODUCIBILITY
#   - Fixed SEED
#   - Log library versions separately if needed
# =============================================================================

import os, re, gc, json, random, warnings, inspect
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from matplotlib.colors import LinearSegmentedColormap

from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder, MaxAbsScaler
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.model_selection import StratifiedKFold
from sklearn.linear_model import LogisticRegression
from sklearn.neural_network import MLPClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.isotonic import IsotonicRegression
from sklearn.calibration import calibration_curve
from sklearn.metrics import (
    roc_auc_score, roc_curve, accuracy_score, f1_score, precision_score,
    recall_score, confusion_matrix, ConfusionMatrixDisplay,
    log_loss, average_precision_score
)

import xgboost as xgb
import lightgbm as lgb
from catboost import CatBoostClassifier
from scipy.optimize import minimize

warnings.filterwarnings("ignore")

# -----------------------------------------------------------------------------
# 0) SAFE TORCH IMPORT (OPTIONAL)
# -----------------------------------------------------------------------------
try:
    import torch as _torch
    if not hasattr(_torch, "cuda"):
        _torch.cuda = type("cuda_ns", (), {"is_available": staticmethod(lambda: False)})()
    HAS_CUDA = bool(_torch.cuda.is_available())
    DEVICE = "cuda" if HAS_CUDA else "cpu"
    torch = _torch
except Exception as e:
    print(f"[warn] PyTorch unavailable or import failed ({e}). Using sklearn-only backbone.")
    torch, HAS_CUDA, DEVICE = None, False, "cpu"

# -----------------------------------------------------------------------------
# 1) PATHS & SEED
# -----------------------------------------------------------------------------
SEED = 42
np.random.seed(SEED); random.seed(SEED)

TRAIN_PATH = os.environ.get("PREDICT_PACA_TRAIN", "./data/Chort_train.csv")
VALID_PATH = os.environ.get("PREDICT_PACA_VALID", "./data/Chort_valid.csv")
OUTDIR     = os.environ.get("PREDICT_PACA_OUTPUTS", "./outputs")
os.makedirs(OUTDIR, exist_ok=True)

# -----------------------------------------------------------------------------
# 2) FEATURE SCHEMA
# -----------------------------------------------------------------------------
RAW_FEATS = [
    "Age","Sex","Ethnicity","Weight",
    "Hypertension","Diabetes Mellitus","Coronary Artery Disease","Valvular Heart Disease","Heart Failure",
    "Blood Urea Nitrogen","Creatinine","Glucose","Chloride","Potassium","Sodium","Total Protein",
    "White Blood Cell Count","Hemoglobin","Heart Rate","Respiratory Rate","End Tidal CO2","Body Temperature"
]
TARGET = "Death 365 Days"

def _norm(s: str) -> str:
    return "".join(ch for ch in str(s).lower().strip() if ch.isalnum())

def map_columns(df: pd.DataFrame, names: list) -> dict:
    # Robust resolver: tolerate casing/spacing/symbol variations.
    act = {_norm(c): c for c in df.columns}
    mp = {}
    for want in names + [TARGET]:
        key = _norm(want)
        if key in act:
            mp[want] = act[key]
        else:
            cands = [c for k, c in act.items() if key in k or k in key]
            if cands:
                mp[want] = cands[0]
            else:
                raise KeyError(f"Cannot find column: {want}")
    return mp

def binarize_target(v):
    # Accepts {1/0, yes/no, true/false, death/alive, etc.}
    if pd.isna(v): return np.nan
    if isinstance(v, str):
        t = v.strip().lower()
        if t in {"1","true","yes","death","dead","deceased"}: return 1
        if t in {"0","false","no","alive","survive","survived"}: return 0
    try:
        return 1 if int(float(v)) == 1 else 0
    except Exception:
        return np.nan

# -----------------------------------------------------------------------------
# 3) LOAD DATA & BASIC CLEANUP
# -----------------------------------------------------------------------------
df_tr = pd.read_csv(TRAIN_PATH)
df_va = pd.read_csv(VALID_PATH)

colmap = map_columns(df_tr, RAW_FEATS)
X_cols = [colmap[c] for c in RAW_FEATS]
y_col  = colmap.get(TARGET, TARGET)

df_tr[y_col] = df_tr[y_col].apply(binarize_target)
df_va[y_col] = df_va[y_col].apply(binarize_target)
df_tr = df_tr.dropna(subset=[y_col]).copy()
df_va = df_va.dropna(subset=[y_col]).copy()

CAT_COLS = [colmap["Sex"], colmap["Ethnicity"]]
NUM_COLS = [c for c in X_cols if c not in CAT_COLS]

for c in NUM_COLS:
    df_tr[c] = pd.to_numeric(df_tr[c], errors="coerce").astype("float32")
    df_va[c] = pd.to_numeric(df_va[c], errors="coerce").astype("float32")

Xtr_df, ytr = df_tr[X_cols].copy(), df_tr[y_col].astype(int).values
Xva_df, yva = df_va[X_cols].copy(), df_va[y_col].astype(int).values

# -----------------------------------------------------------------------------
# 4) PREPROCESSING (NUM: median+MaxAbs, CAT: OHE)
# -----------------------------------------------------------------------------
num_tf = Pipeline([
    ("imputer", SimpleImputer(strategy="median")),
    ("scaler",  MaxAbsScaler())
])

try:
    cat_tf = OneHotEncoder(handle_unknown="ignore", sparse_output=True, dtype=np.float32)
except TypeError:  # sklearn < 1.2
    cat_tf = OneHotEncoder(handle_unknown="ignore", sparse=True, dtype=np.float32)

preprocess_ohe = ColumnTransformer(
    [("num", num_tf, NUM_COLS), ("cat", cat_tf, CAT_COLS)],
    sparse_threshold=1.0
)

# -----------------------------------------------------------------------------
# 5) METRICS & CALIBRATION HELPERS
# -----------------------------------------------------------------------------
def expected_calibration_error(y_true, y_prob, n_bins=15):
    y_true = np.asarray(y_true).ravel()
    y_prob = np.asarray(y_prob).ravel()
    bins = np.linspace(0, 1, n_bins + 1)
    ece = 0.0
    for i in range(n_bins):
        lo, hi = bins[i], bins[i+1]
        idx = (y_prob >= lo) & (y_prob < (hi if i < n_bins-1 else hi + 1e-12))
        if idx.any():
            conf = y_prob[idx].mean()
            acc  = y_true[idx].mean()
            ece += idx.mean() * abs(conf - acc)
    return float(ece)

def brier_score(y_true, y_prob):
    y_true = np.asarray(y_true).astype(float)
    y_prob = np.asarray(y_prob).astype(float)
    return float(np.mean((y_prob - y_true) ** 2))

def bin_metrics(y_true, y_prob, thr=0.5):
    yhat = (np.asarray(y_prob) >= thr).astype(int)
    return dict(
        recall    = recall_score(y_true, yhat, zero_division=0),
        accuracy  = accuracy_score(y_true, yhat),
        f1        = f1_score(y_true, yhat, zero_division=0),
        precision = precision_score(y_true, yhat, zero_division=0),
        ece       = expected_calibration_error(y_true, y_prob, 15),
        log_loss  = log_loss(y_true, np.clip(y_prob, 1e-6, 1-1e-6)),
        brier     = brier_score(y_true, y_prob),
        auc       = roc_auc_score(y_true, y_prob),
        auprc     = average_precision_score(y_true, y_prob),
    )

def find_thresholds(y_true, y_prob):
    thr_grid = np.linspace(0.05, 0.95, 181)
    best_f1 = (0.0, 0.5)
    best_rec_pre55 = (0.0, 0.5)
    fpr, tpr, roc_thr = roc_curve(y_true, y_prob)
    youden_thr = float(roc_thr[np.argmax(tpr - fpr)])

    from sklearn.metrics import precision_recall_fscore_support
    for t in thr_grid:
        yhat = (y_prob >= t).astype(int)
        p, r, f, _ = precision_recall_fscore_support(
            y_true, yhat, average="binary", zero_division=0
        )
        if f > best_f1[0]: best_f1 = (f, t)
        if p >= 0.55 and r > best_rec_pre55[0]: best_rec_pre55 = (r, t)

    return {
        "best_f1_thr": best_f1[1],
        "recall@p>=0.55_thr": best_rec_pre55[1],
        "youden_thr": youden_thr
    }

def safe_logit(p, eps=1e-6):
    p = np.clip(p, eps, 1 - eps)
    return np.log(p) - np.log(1 - p)

def temperature_scale_fit(p_train, y_train):
    eps = 1e-12
    p_raw = np.clip(p_train, eps, 1 - eps)
    lg = np.log(p_raw) - np.log(1 - p_raw)

    def nll(T):
        z = lg / T[0]
        p = 1 / (1 + np.exp(-z))
        p = np.clip(p, eps, 1 - eps)
        return -np.mean(y_train * np.log(p) + (1 - y_train) * np.log(1 - p))

    res = minimize(nll, x0=np.array([1.0]), bounds=[(0.05, 5.0)])
    T = float(res.x[0])

    def apply(p_in):
        p_in = np.clip(p_in, eps, 1 - eps)
        z = (np.log(p_in) - np.log(1 - p_in)) / T
        return 1 / (1 + np.exp(-z))

    return apply, T

def cv_temperature_scaling(p_oof, y_tr, n_splits=5):
    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=SEED)
    Ts = []
    for tr_idx, _ in skf.split(p_oof.reshape(-1, 1), y_tr):
        cal, T = temperature_scale_fit(p_oof[tr_idx], y_tr[tr_idx])
        Ts.append(T)
    Tbar = float(np.mean(Ts))

    def apply(p):
        eps = 1e-12
        p = np.clip(p, eps, 1 - eps)
        z = (np.log(p) - np.log(1 - p)) / Tbar
        return 1 / (1 + np.exp(-z))

    return apply, Tbar

# -----------------------------------------------------------------------------
# 6) CLASS IMBALANCE HANDLING
# -----------------------------------------------------------------------------
pos = ytr.sum(); neg = len(ytr) - pos
SCALE_POS    = float(neg / max(1, pos))  # trees/XGB/LGB/Cat
CLASS_WEIGHT = "balanced"                # LR/RF

def class_balanced_weights(y, beta=0.999):
    y = np.asarray(y).astype(int)
    n_pos = y.sum(); n_neg = len(y) - n_pos
    eff_pos = (1 - beta) / (1 - beta ** max(1, n_pos))
    eff_neg = (1 - beta) / (1 - beta ** max(1, n_neg))
    w = np.where(y == 1, eff_pos, eff_neg)
    return w / np.mean(w)

sample_weight_tr = class_balanced_weights(ytr)

def supports_sample_weight(estimator) -> bool:
    sig = inspect.signature(estimator.fit)
    return "sample_weight" in sig.parameters

# -----------------------------------------------------------------------------
# 7) BASELINE MODEL FACTORIES
# -----------------------------------------------------------------------------
def make_LR(class_weight=CLASS_WEIGHT):
    return Pipeline([
        ("pre", preprocess_ohe),
        ("clf", LogisticRegression(
            max_iter=800, solver="lbfgs", C=1.5,
            class_weight=class_weight
        ))
    ])

def make_RF():
    return Pipeline([
        ("pre", preprocess_ohe),
        ("clf", RandomForestClassifier(
            n_estimators=800, max_depth=None, max_features=0.6,
            min_samples_split=4, min_samples_leaf=1,
            class_weight="balanced_subsample", n_jobs=-1,
            random_state=SEED
        ))
    ])

def make_XGB(scale_pos=SCALE_POS):
    return Pipeline([
        ("pre", preprocess_ohe),
        ("clf", xgb.XGBClassifier(
            n_estimators=800, max_depth=5, learning_rate=0.03,
            subsample=0.8, colsample_bytree=0.8,
            min_child_weight=3, reg_alpha=0.2, reg_lambda=2.0,
            scale_pos_weight=scale_pos,
            eval_metric="aucpr", tree_method="hist",
            random_state=SEED
        ))
    ])

def make_LGB(scale_pos=SCALE_POS):
    return Pipeline([
        ("pre", preprocess_ohe),
        ("clf", lgb.LGBMClassifier(
            n_estimators=600, num_leaves=48, learning_rate=0.03,
            min_child_samples=35, feature_fraction=0.85,
            bagging_fraction=0.8, bagging_freq=1,
            lambda_l1=0.0, lambda_l2=5.0,
            objective="binary", metric=["binary_logloss","auc"],
            scale_pos_weight=scale_pos,
            random_state=SEED
        ))
    ])

def make_CAT(scale_pos=SCALE_POS):
    # Use unified OHE flow for reproducibility across environments.
    return Pipeline([
        ("pre", preprocess_ohe),
        ("clf", CatBoostClassifier(
            iterations=800, depth=6, learning_rate=0.03,
            l2_leaf_reg=3.0, loss_function="Logloss", eval_metric="AUC",
            random_seed=SEED, verbose=False,
            scale_pos_weight=scale_pos, allow_writing_files=False
        ))
    ])

def make_MLP():
    return Pipeline([
        ("pre", preprocess_ohe),
        ("clf", MLPClassifier(
            hidden_layer_sizes=(256,128,64), activation="relu",
            alpha=5e-4, batch_size=64, learning_rate_init=5e-4,
            learning_rate="adaptive", early_stopping=True,
            n_iter_no_change=20, validation_fraction=0.1,
            max_iter=600, random_state=SEED
        ))
    ])

# -----------------------------------------------------------------------------
# 8) OOF TRAINING (SEED + SAMPLE_WEIGHT AWARE)
# -----------------------------------------------------------------------------
def oof_raw(est_maker, name, X, y, X_valid, n_splits=5, seed=SEED, samp_weight=None):
    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)
    oof = np.zeros(len(X), dtype=float)

    for tr_idx, va_idx in skf.split(X, y):
        est = est_maker()
        sw = samp_weight[tr_idx] if samp_weight is not None else None

        if isinstance(est, Pipeline):
            last = list(est.named_steps.keys())[-1]
            final_est = est.named_steps[last]
            if supports_sample_weight(final_est) and sw is not None:
                est.fit(X.iloc[tr_idx], y[tr_idx], **{f"{last}__sample_weight": sw})
            else:
                est.fit(X.iloc[tr_idx], y[tr_idx])
        else:
            if supports_sample_weight(est) and sw is not None:
                est.fit(X.iloc[tr_idx], y[tr_idx], sample_weight=sw)
            else:
                est.fit(X.iloc[tr_idx], y[tr_idx])

        oof[va_idx] = est.predict_proba(X.iloc[va_idx])[:, 1]
        del est; gc.collect()

    est_full = est_maker()
    if isinstance(est_full, Pipeline):
        last = list(est_full.named_steps.keys())[-1]
        final_est = est_full.named_steps[last]
        if supports_sample_weight(final_est) and samp_weight is not None:
            est_full.fit(X, y, **{f"{last}__sample_weight": samp_weight})
        else:
            est_full.fit(X, y)
    else:
        if supports_sample_weight(est_full) and samp_weight is not None:
            est_full.fit(X, y, sample_weight=samp_weight)
        else:
            est_full.fit(X, y)

    valid_prob = est_full.predict_proba(X_valid)[:, 1]
    del est_full; gc.collect()
    return oof, valid_prob

# -----------------------------------------------------------------------------
# 9) TEACHER DISTILLATION (XGB + LGB + LR) â†’ teacher_logit feature
# -----------------------------------------------------------------------------
teachers = {"XGB": make_XGB, "LGB": make_LGB, "LR": make_LR}
oof_logits, val_logits = [], []

for tname, maker in teachers.items():
    oof_t, val_t = oof_raw(maker, tname, Xtr_df, ytr, Xva_df,
                           n_splits=5, seed=SEED, samp_weight=sample_weight_tr)
    oof_logits.append(safe_logit(oof_t))
    val_logits.append(safe_logit(val_t))

O = np.vstack(oof_logits).T  # (n_train, K)
V = np.vstack(val_logits).T  # (n_valid, K)

teach_lr = LogisticRegression(max_iter=3000, solver="lbfgs")
teach_lr.fit(O, ytr)
teacher_logit_oof = (O @ teach_lr.coef_.ravel()) + teach_lr.intercept_[0]
teacher_logit_val = (V @ teach_lr.coef_.ravel()) + teach_lr.intercept_[0]

Xtr_df_distill = Xtr_df.copy();  Xva_df_distill = Xva_df.copy()
Xtr_df_distill["teacher_logit"] = teacher_logit_oof.astype("float32")
Xva_df_distill["teacher_logit"] = teacher_logit_val.astype("float32")

# -----------------------------------------------------------------------------
# 10) PyTabKit BACKBONE (OPTIONAL) â€” FALLBACK TO SKLEARN MLP
# -----------------------------------------------------------------------------
def import_pytabkit_model():
    try:
        from pytabkit import RealMLP_TD_Classifier
        return "RealMLP_TD_Classifier", RealMLP_TD_Classifier
    except Exception:
        pass
    try:
        from pytabkit.models import RealMLP_TD_Classifier
        return "RealMLP_TD_Classifier", RealMLP_TD_Classifier
    except Exception:
        return None, None

model_name, PTKModel = import_pytabkit_model()
if PTKModel is None or torch is None:
    print("[warn] PyTabKit or torch not available; falling back to sklearn MLP.")
    pre2 = ColumnTransformer(
        [
            ("num", Pipeline([("imputer", SimpleImputer(strategy="median")),
                              ("scaler", MaxAbsScaler())]),
             NUM_COLS + ["teacher_logit"]),
            ("cat", cat_tf, CAT_COLS)
        ],
        sparse_threshold=1.0
    )
    clf = Pipeline([
        ("pre", pre2),
        ("clf", MLPClassifier(
            hidden_layer_sizes=(512, 256, 128),
            activation="relu", alpha=1e-5,
            learning_rate_init=1e-3, learning_rate="adaptive",
            early_stopping=True, n_iter_no_change=20,
            validation_fraction=0.1, max_iter=200, random_state=SEED
        ))
    ])
    clf.fit(Xtr_df_distill, ytr)
    p_ptk = clf.predict_proba(Xva_df_distill)[:, 1]
    ptk_params = {"backbone": "sklearn_mlp"}
    model_name = model_name or "sklearn-MLP"
else:
    def prep_for_ptk(df_src):
        df = df_src.copy()
        for c in NUM_COLS + (["teacher_logit"] if "teacher_logit" in df.columns else []):
            med = pd.to_numeric(Xtr_df_distill[c], errors="coerce").median()
            df[c] = pd.to_numeric(df[c], errors="coerce").fillna(med).astype("float32")
        for c in CAT_COLS:
            df[c] = df[c].astype(str).fillna("NA")
        return df

    Xtr_ptk = prep_for_ptk(Xtr_df_distill)
    Xva_ptk = prep_for_ptk(Xva_df_distill)

    ptk_params = {
        "random_state": SEED, "n_cv": 1, "epochs": 200, "batch_size": 256,
        "lr": 1e-3, "dropout": 0.1, "weight_decay": 1e-5, "batch_norm": True,
        "hidden_dims": [512, 256, 128], "pos_weight": float(SCALE_POS),
        "calibration_method": None
    }
    valid_keys = set(inspect.signature(PTKModel.__init__).parameters.keys())
    ptk_params = {k: v for k, v in ptk_params.items() if k in valid_keys}

    PTK = PTKModel(**ptk_params)
    try:
        PTK.fit(Xtr_ptk, ytr, X_val=Xva_ptk, y_val=yva, cat_col_names=CAT_COLS)
    except TypeError:
        try:
            PTK.fit(Xtr_ptk, ytr, cat_col_names=CAT_COLS)
        except TypeError:
            PTK.fit(Xtr_ptk, ytr)
    p_ptk = PTK.predict_proba(Xva_ptk)[:, 1]

# -----------------------------------------------------------------------------
# 11) BASE LEARNERS FOR STACKING/BLENDING
# -----------------------------------------------------------------------------
base_makers = {
    "XGBoost":       make_XGB,
    "CatBoost":      make_CAT,
    "LightGBM":      make_LGB,
    "MLP":           make_MLP,
    "Random forest": make_RF,
    "LR":            make_LR
}
oof_mat, valid_mat = [], []
probs = {"PyTabKit-Net": p_ptk}
params_rows = [{"model": f"PyTabKit-Net ({model_name})", **(ptk_params if isinstance(ptk_params, dict) else {})}]

for name, maker in base_makers.items():
    oof_b, val_b = oof_raw(maker, name, Xtr_df, ytr, Xva_df,
                           n_splits=5, seed=SEED, samp_weight=sample_weight_tr)
    oof_mat.append(oof_b); valid_mat.append(val_b)
    probs[name] = val_b
    params_rows.append({"model": name})

oof_mat   = np.vstack(oof_mat).T   # (n_train, K)
valid_mat = np.vstack(valid_mat).T # (n_valid, K)

# -----------------------------------------------------------------------------
# 12) SPARSE STACKING (L1) + SOFT VOTING WITH CALIBRATION
# -----------------------------------------------------------------------------
meta_name = "StackingMeta"
meta = LogisticRegression(penalty="l1", solver="liblinear", C=0.3, max_iter=5000)
meta.fit(oof_mat, ytr)
p_stack = meta.predict_proba(valid_mat)[:, 1]
probs[meta_name] = p_stack
params_rows.append({"model": f"{meta_name}(L1)", "base_models": list(base_makers.keys())})

def optimize_weights(prob_dict, y_true,
                     base_models=("PyTabKit-Net","StackingMeta","XGBoost","LR","LightGBM"),
                     n=3000, lam=15.0, seed=SEED):
    P = np.vstack([prob_dict[m] for m in base_models]).T
    rng = np.random.default_rng(seed)
    eces  = [max(1e-4, expected_calibration_error(y_true, prob_dict[m])) for m in base_models]
    prior = np.array([1.0 / e for e in eces], dtype=float)
    for i, m in enumerate(base_models):
        if "XGBoost" in m: prior[i] *= 0.6  # mild downweight on prior
    best = (1e18, None)
    for _ in range(n):
        w = rng.dirichlet(prior)
        p = np.clip(P.dot(w), 1e-9, 1-1e-9)
        score = log_loss(y_true, p) + lam * expected_calibration_error(y_true, p)
        if score < best[0]: best = (score, w)
    return base_models, best[1], float(best[0])

ens_names, ens_w, ens_obj = optimize_weights(probs, yva)
P = np.vstack([probs[m] for m in ens_names]).T
p_ens = np.clip(P.dot(ens_w), 1e-6, 1-1e-6)

with open(os.path.join(OUTDIR, "ensemble_weights.json"), "w") as f:
    json.dump(
        {"models": ens_names, "weights": ens_w.tolist(),
         "obj_logloss_plus_lamECE": float(ens_obj)},
        f, indent=2
    )

# Blend OOFs to fit calibrators (use only learners with OOF available)
name2oof = {nm: oof_mat[:, list(base_makers.keys()).index(nm)] for nm in base_makers.keys()}
oof_base = [m for m in ens_names if m in name2oof]
if len(oof_base) >= 2:
    oofP = np.vstack([name2oof[m] for m in oof_base]).T
    idx  = np.array([ens_names.index(m) for m in oof_base])
    w_oof = ens_w[idx] / (ens_w[idx].sum() + 1e-12)
    p_oof_ens = np.clip(oofP.dot(w_oof), 1e-6, 1-1e-6)
else:
    single = oof_base[0] if len(oof_base) == 1 else ens_names[0]
    p_oof_ens = name2oof.get(single, probs["PyTabKit-Net"])

# CV-TS vs Isotonic â€” pick better Brier
ts_apply, _T = cv_temperature_scaling(p_oof_ens, ytr, n_splits=5)
p_valid_ts   = ts_apply(p_ens)
iso          = IsotonicRegression(out_of_bounds="clip"); iso.fit(p_oof_ens, ytr)
p_valid_iso  = iso.predict(p_ens)
b_ts, b_iso  = brier_score(yva, p_valid_ts), brier_score(yva, p_valid_iso)
p_final      = p_valid_ts if b_ts <= b_iso else p_valid_iso
probs["EnsembleSoft_cal"] = p_final
print(f"[cal] choose {'CV-TS' if b_ts<=b_iso else 'Isotonic'}  (Brier TS={b_ts:.6f}, ISO={b_iso:.6f})")

# -----------------------------------------------------------------------------
# 13) METRICS TABLES + PLOTS (NAVY THEME, 400 DPI)
# -----------------------------------------------------------------------------
def metrics_with_thresholds(y_true, y_prob):
    thrs = find_thresholds(y_true, y_prob)
    out = {}
    for tag, thr in {
        "@0.5": 0.5,
        "@bestF1": thrs["best_f1_thr"],
        "@R@P>=0.55": thrs["recall@p>=0.55_thr"],
        "@Youden": thrs["youden_thr"]
    }.items():
        out[tag] = bin_metrics(y_true, y_prob, thr=float(thr))
    return out

rows = []
for name, p in probs.items():
    allm = metrics_with_thresholds(yva, p)
    for tag, m in allm.items():
        rows.append({"model": name + tag, **m})

metrics_df = pd.DataFrame(rows).sort_values(["auc", "f1"], ascending=[False, False])
params_df  = pd.DataFrame(params_rows)
metrics_df.to_csv(os.path.join(OUTDIR, "metrics_table.csv"), index=False)
params_df.to_csv(os.path.join(OUTDIR, "params_table.csv"), index=False)
print("\n=== Metrics on VALID (multi-threshold) ===")
print(metrics_df.to_string(index=False))

# Navy blues for confusion matrices
_CONF_CMAP = LinearSegmentedColormap.from_list(
    "navy_blues", ["#F1F6FF", "#AEC7FF", "#6B93D6", "#0B2E5B"]
)

def _safe_name(s: str) -> str:
    return re.sub(r"[^A-Za-z0-9_.-]+", "_", str(s))

def save_confusion_blue(y_true, y_prob, title, thr=0.5, outdir=OUTDIR):
    yhat = (np.asarray(y_prob) >= thr).astype(int)
    cm   = confusion_matrix(y_true, yhat)
    fig, ax = plt.subplots(figsize=(5.6, 6.0), dpi=400)
    disp = ConfusionMatrixDisplay(confusion_matrix=cm)
    disp.plot(ax=ax, colorbar=False, cmap=_CONF_CMAP, values_format="")
    for (i, j), v in np.ndenumerate(cm):
        ax.text(j, i, f"{int(v)}", ha="center", va="center",
                fontsize=16, color="#0B2E5B", fontweight="semibold")
    ax.set_title(title, fontsize=20, pad=12)
    ax.set_xlabel("Predicted label", fontsize=13)
    ax.set_ylabel("True label", fontsize=13)
    plt.tight_layout()
    fig.savefig(os.path.join(outdir, f"confusion_{_safe_name(title)}.png"))
    plt.close(fig)

for name, p in probs.items():
    save_confusion_blue(yva, p, name, thr=0.5)

# ROC â€” only the six specified models
plt.figure(figsize=(8.5, 7.5), dpi=400)
_selected = ["PyTabKit-Net","XGBoost","CatBoost","LightGBM","MLP","Random forest"]
for name in _selected:
    if name not in probs:
        print(f"[info] skip missing model: {name}")
        continue
    p = probs[name]
    fpr, tpr, _ = roc_curve(yva, p); auc = roc_auc_score(yva, p)
    plt.plot(fpr, tpr, label=f"{name} (AUC={auc:.3f})")
plt.plot([0,1],[0,1],"--",color="gray")
plt.xlabel("False Positive Rate"); plt.ylabel("True Positive Rate")
plt.title("ROC Curves")
plt.legend(ncol=2, fontsize=9)
plt.tight_layout(); plt.savefig(os.path.join(OUTDIR, "roc_all.png")); plt.close()

# Calibration â€” same six models
plt.figure(figsize=(7.8, 7.8), dpi=400)
for name in _selected:
    if name not in probs:
        continue
    p = probs[name]
    pt, pp = calibration_curve(yva, p, n_bins=15, strategy="uniform")
    plt.plot(pp, pt, marker="o", linewidth=1, label=name)
plt.plot([0,1],[0,1],"--",color="gray")
plt.xlabel("Predicted probability"); plt.ylabel("Empirical frequency")
plt.title("Calibration")
plt.legend(ncol=2, fontsize=8)
plt.tight_layout(); plt.savefig(os.path.join(OUTDIR, "calibration_all.png")); plt.close()

print(f"\nAll outputs saved to: {OUTDIR}")